I guess nobody has tested it :-)

I'd speculate that pre-compiling wouldn't offer enough extra
performance to justify the effort and I'd like to explain why.

For the sake of an example, let's say that pre-compiled code reduced
startup time for a task by 5% and that you had a job that got divided
up into 10,000 individual tasks.

It might first seem like the time savings would really add up since
there are so many tasks. But when you consider that it might only take
one second for Python to compile the code, the 5% savings amounts to
50 milliseconds. And the total observed savings is also not
necessarily 500 seconds (i.e. 50 milliseconds * 10,000 tasks), because
these tasks are independent and many of them will be running in
parallel.

In summary, if the task is short-lived enough that the time savings
from pre-compiling the code is a significant part of the task's total
runtime, then the task is probably not processing enough data.
Increasing the HDFS block size for the input data would likely
increase performance in this case.


On Fri, Jun 7, 2013 at 4:04 PM, Jason Levitt
<jason.levitt@globalknowledge.com> wrote:
> The question came up whether compiling Python for streaming API
> MR would provide any performance boost over non-compiled Python.
> It seems like it might if the Map or Reduce app was short-lived enough (and then
> the increased start-up time might be valuable).  But I've never
> seen any numbers to justify that. Anyone tested it?
>
> J

